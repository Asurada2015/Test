C:\ProgramData\Anaconda3\envs\py35gpu\python.exe "D:/CODE/Git/Test/Prediction based on convolutional neural network/code/NewModels1/Tensile_BNalpha.py"
2018-05-29 17:01:15.408287: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2018-05-29 17:01:15.677650: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 750 Ti major: 5 minor: 0 memoryClockRate(GHz): 1.0845
pciBusID: 0000:01:00.0
totalMemory: 4.00GiB freeMemory: 3.35GiB
2018-05-29 17:01:15.678035: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0, compute capability: 5.0)
Now getting and transforming Data
Creating the CNN model.
Declare Loss Function.
Create the Train Operation
Initializing the Variables.
Starting Training
  0%|          | 150/50002 [02:49<19:18:14,  1.39s/it]Generation 150: train Loss = 0.14518
Generation 150: test Loss = 0.13203
  0%|          | 199/50002 [03:43<15:01:22,  1.09s/it]训练集批次数据的前10个数据
 [[ 0.39285713]
 [ 0.4017857 ]
 [ 0.5714286 ]
 [ 0.5535714 ]
 [ 0.5089286 ]
 [ 0.5625    ]
 [ 0.5625    ]
 [ 0.51785713]
  0%|          | 200/50002 [03:45<19:09:59,  1.39s/it] [ 0.66071427]
 [ 0.66071427]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.446428  ]
 [ 0.536125  ]
 [ 0.53851229]
 [ 0.54049802]
 [ 0.42657977]
 [ 0.48138598]
 [ 0.54820746]
 [ 0.51838386]
 [ 0.45375609]
 [ 0.54982853]]
测试集批次数据的前10个数据
 [[ 0.58035713]
 [ 0.66964287]
 [ 0.65178573]
 [ 0.45535713]
 [ 0.44642857]
 [ 0.59821427]
 [ 0.91964287]
 [ 0.6875    ]
 [ 0.58928573]
 [ 0.79464287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.53001255]
 [ 0.54490209]
 [ 0.52220726]
 [ 0.51017523]
 [ 0.53103161]
 [ 0.47759318]
 [ 0.5110026 ]
 [ 0.57395113]
 [ 0.54994869]
 [ 0.54213065]]
Generation 200: train Loss = 0.12266
Generation 200: test Loss = 0.14147
  0%|          | 250/50002 [04:40<19:02:16,  1.38s/it]Generation 250: train Loss = 0.13570
Generation 250: test Loss = 0.12770
  1%|          | 300/50002 [05:35<19:06:14,  1.38s/it]训练集批次数据的前10个数据
 [[ 0.51785713]
 [ 0.6339286 ]
 [ 0.3125    ]
 [ 0.41964287]
 [ 0.54464287]
 [ 0.73214287]
 [ 0.58035713]
 [ 0.53571427]
 [ 0.41964287]
 [ 0.52678573]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.56070423]
 [ 0.56106961]
 [ 0.49845934]
 [ 0.47054505]
 [ 0.40287021]
 [ 0.53456873]
 [ 0.48081404]
 [ 0.51041907]
 [ 0.3878473 ]
 [ 0.51434147]]
测试集批次数据的前10个数据
 [[ 0.5714286 ]
 [ 0.6339286 ]
 [ 0.16964285]
 [ 0.5       ]
 [ 0.54464287]
 [ 0.3392857 ]
 [ 0.30357143]
 [ 0.48214287]
 [ 0.5535714 ]
 [ 0.66964287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.48557854]
 [ 0.5730387 ]
 [ 0.43108147]
 [ 0.46961582]
 [ 0.54607385]
 [ 0.40322679]
 [ 0.52932733]
 [ 0.47846249]
 [ 0.47718036]
 [ 0.52693403]]
Generation 300: train Loss = 0.11798
Generation 300: test Loss = 0.12697
  1%|          | 349/50002 [06:28<15:01:00,  1.09s/it]Generation 350: train Loss = 0.11054
Generation 350: test Loss = 0.11579
  1%|          | 399/50002 [07:23<14:54:15,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.66964287]
 [ 0.48214287]
 [ 0.5089286 ]
 [ 0.6160714 ]
 [ 0.48214287]
 [ 0.38392857]
 [ 0.5535714 ]
 [ 0.45535713]
 [ 0.58035713]
 [ 0.3482143 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.59937382]
 [ 0.61501336]
 [ 0.58897042]
 [ 0.55266404]
 [ 0.57039851]
 [ 0.42377779]
 [ 0.45446891]
 [ 0.55673718]
 [ 0.453316  ]
 [ 0.43223989]]
测试集批次数据的前10个数据
 [[ 0.5089286 ]
 [ 0.64285713]
 [ 0.5625    ]
 [ 0.35714287]
 [ 0.78571427]
 [ 0.49107143]
 [ 0.5714286 ]
 [ 0.6160714 ]
 [ 0.5535714 ]
 [ 0.71428573]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.47120154]
 [ 0.60920841]
 [ 0.56874102]
 [ 0.45682144]
 [ 0.60112309]
 [ 0.57017219]
 [ 0.53674126]
 [ 0.44651991]
 [ 0.42916593]
 [ 0.6477859 ]]
Generation 400: train Loss = 0.11707
Generation 400: test Loss = 0.11373
  1%|          | 450/50002 [08:21<18:59:42,  1.38s/it]Generation 450: train Loss = 0.10892
Generation 450: test Loss = 0.10424
  1%|          | 500/50002 [09:16<18:55:03,  1.38s/it]训练集批次数据的前10个数据
 [[ 0.25892857]
 [ 0.6785714 ]
 [ 0.48214287]
 [ 0.5089286 ]
 [ 0.5535714 ]
 [ 0.76785713]
 [ 0.71428573]
 [ 0.66964287]
 [ 0.52678573]
 [ 0.58928573]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.5152939 ]
 [ 0.62440646]
 [ 0.47666585]
 [ 0.60929549]
 [ 0.55511588]
 [ 0.65234017]
 [ 0.46145749]
 [ 0.65265501]
 [ 0.59197366]
 [ 0.5579772 ]]
测试集批次数据的前10个数据
 [[ 0.6964286 ]
 [ 0.6160714 ]
 [ 0.65178573]
 [ 0.58035713]
 [ 0.64285713]
 [ 0.38392857]
 [ 0.29464287]
 [ 0.30357143]
 [ 0.60714287]
 [ 0.5535714 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.60507405]
 [ 0.54591256]
 [ 0.58569461]
 [ 0.62164021]
 [ 0.50716984]
 [ 0.49533781]
 [ 0.61685598]
 [ 0.6139822 ]
 [ 0.69105875]
 [ 0.55422986]]
Generation 500: train Loss = 0.10865
Generation 500: test Loss = 0.11091
  1%|          | 549/50002 [10:09<15:01:37,  1.09s/it]Generation 550: train Loss = 0.10377
Generation 550: test Loss = 0.09844
  1%|          | 599/50002 [11:04<14:55:49,  1.09s/it]训练集批次数据的前10个数据
 [[ 0.66964287]
 [ 0.51785713]
 [ 0.5535714 ]
 [ 0.52678573]
 [ 0.7589286 ]
 [ 0.5625    ]
 [ 0.5535714 ]
 [ 0.54464287]
 [ 0.41964287]
 [ 0.5535714 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.61353254]
 [ 0.47886154]
 [ 0.56821156]
 [ 0.41838127]
 [ 0.69762969]
 [ 0.47339049]
 [ 0.54615676]
 [ 0.54118764]
 [ 0.43890563]
 [ 0.62209928]]
测试集批次数据的前10个数据
 [[ 0.4107143 ]
 [ 0.54464287]
 [ 0.44642857]
 [ 0.60714287]
 [ 0.54464287]
 [ 0.58928573]
 [ 0.41964287]
 [ 0.625     ]
 [ 0.51785713]
 [ 0.59821427]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.62279177]
 [ 0.61373091]
 [ 0.5917666 ]
 [ 0.50423026]
 [ 0.59196186]
 [ 0.608226  ]
 [ 0.50104982]
 [ 0.50231755]
 [ 0.437702  ]
 [ 0.52137184]]
Generation 600: train Loss = 0.09968
Generation 600: test Loss = 0.11381
  1%|         | 650/50002 [12:01<19:17:52,  1.41s/it]Generation 650: train Loss = 0.10399
Generation 650: test Loss = 0.09765
  1%|         | 700/50002 [12:56<18:43:21,  1.37s/it]训练集批次数据的前10个数据
 [[ 0.4375    ]
 [ 0.4375    ]
 [ 0.6160714 ]
 [ 0.14285715]
 [ 0.6875    ]
 [ 0.76785713]
 [ 0.25892857]
 [ 0.4375    ]
 [ 0.85714287]
 [ 0.49107143]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.50089657]
 [ 0.48466057]
 [ 0.60378468]
 [ 0.43910846]
 [ 0.62690949]
 [ 0.60716045]
 [ 0.45839936]
 [ 0.44415897]
 [ 0.67121887]
 [ 0.5278272 ]]
测试集批次数据的前10个数据
 [[ 0.51785713]
 [ 0.8125    ]
 [ 0.59821427]
 [ 0.71428573]
 [ 0.4732143 ]
 [ 0.44642857]
 [ 0.66964287]
 [ 0.58928573]
 [ 0.5       ]
 [ 0.39285713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.39580929]
 [ 0.61750394]
 [ 0.47771132]
 [ 0.62873578]
 [ 0.53235793]
 [ 0.69827682]
 [ 0.60947156]
 [ 0.58126861]
 [ 0.4770875 ]
 [ 0.51112729]]
Generation 700: train Loss = 0.09430
Generation 700: test Loss = 0.10227
  1%|         | 750/50002 [13:51<18:44:49,  1.37s/it]Generation 750: train Loss = 0.09292
Generation 750: test Loss = 0.09538
  2%|         | 799/50002 [14:44<15:06:38,  1.11s/it]训练集批次数据的前10个数据
 [[ 0.36607143]
 [ 0.8214286 ]
 [ 0.64285713]
 [ 0.6875    ]
 [ 0.51785713]
 [ 0.4107143 ]
 [ 0.6339286 ]
 [ 0.6160714 ]
 [ 0.66964287]
 [ 0.66964287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.49729037]
 [ 0.639171  ]
 [ 0.64299786]
 [ 0.66961479]
 [ 0.47561514]
 [ 0.35696372]
 [ 0.61545622]
 [ 0.5493412 ]
 [ 0.54317641]
 [ 0.60138547]]
测试集批次数据的前10个数据
 [[ 0.36607143]
 [ 0.4732143 ]
 [ 0.58928573]
 [ 0.83035713]
 [ 0.58928573]
 [ 0.6160714 ]
 [ 0.66071427]
 [ 0.52678573]
 [ 0.35714287]
 [ 0.48214287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.5757702 ]
 [ 0.54199529]
 [ 0.5506689 ]
 [ 0.60569525]
 [ 0.5685066 ]
 [ 0.59743291]
 [ 0.64060098]
 [ 0.51155019]
 [ 0.33850455]
 [ 0.52414083]]
Generation 800: train Loss = 0.10441
Generation 800: test Loss = 0.11139
  2%|         | 849/50002 [15:39<14:43:23,  1.08s/it]Generation 850: train Loss = 0.09179
Generation 850: test Loss = 0.09976
  2%|         | 900/50002 [16:36<18:49:44,  1.38s/it]训练集批次数据的前10个数据
 [[ 0.4017857 ]
 [ 0.5089286 ]
 [ 0.4642857 ]
 [ 0.42857143]
 [ 0.5625    ]
 [ 0.5089286 ]
 [ 0.51785713]
 [ 0.66071427]
 [ 0.72321427]
 [ 0.4642857 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.54554021]
 [ 0.52246571]
 [ 0.43062469]
 [ 0.43942931]
 [ 0.60645628]
 [ 0.6919018 ]
 [ 0.54499996]
 [ 0.61939585]
 [ 0.63270211]
 [ 0.44824311]]
测试集批次数据的前10个数据
 [[ 0.8125    ]
 [ 0.24107143]
 [ 0.51785713]
 [ 0.65178573]
 [ 0.7410714 ]
 [ 0.5625    ]
 [ 0.65178573]
 [ 0.625     ]
 [ 0.52678573]
 [ 0.58928573]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.61556506]
 [ 0.53015578]
 [ 0.50978243]
 [ 0.60044044]
 [ 0.61063731]
 [ 0.47686344]
 [ 0.51404917]
 [ 0.58046013]
 [ 0.55428708]
 [ 0.57143599]]
Generation 900: train Loss = 0.10244
Generation 900: test Loss = 0.09173
  2%|         | 950/50002 [17:31<18:54:00,  1.39s/it]Generation 950: train Loss = 0.09664
Generation 950: test Loss = 0.09648
  2%|         | 999/50002 [18:24<14:36:36,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.51785713]
 [ 0.44642857]
 [ 0.54464287]
 [ 0.59821427]
 [ 0.5       ]
 [ 0.48214287]
 [ 0.41964287]
 [ 0.51785713]
 [ 0.5535714 ]
 [ 0.5714286 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.60399747]
 [ 0.46260232]
 [ 0.64115405]
 [ 0.56876636]
 [ 0.51362014]
 [ 0.36074984]
 [ 0.55963624]
 [ 0.52286255]
 [ 0.48623377]
 [ 0.6084491 ]]
测试集批次数据的前10个数据
 [[ 0.625     ]
 [ 0.59821427]
 [ 0.6785714 ]
 [ 0.53571427]
 [ 0.52678573]
 [ 0.6339286 ]
 [ 0.4732143 ]
 [ 0.65178573]
 [ 0.38392857]
 [ 0.5535714 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.61737162]
 [ 0.62857521]
 [ 0.60099554]
 [ 0.46124804]
 [ 0.54770744]
 [ 0.48147038]
 [ 0.52670795]
 [ 0.55565482]
 [ 0.44873399]
 [ 0.54710358]]
Generation 1000: train Loss = 0.10724
Generation 1000: test Loss = 0.09504
  2%|         | 1050/50002 [19:21<18:51:32,  1.39s/it]Generation 1050: train Loss = 0.10057
Generation 1050: test Loss = 0.10319
  2%|         | 1100/50002 [20:16<18:49:04,  1.39s/it]训练集批次数据的前10个数据
 [[ 0.3482143 ]
 [ 0.5535714 ]
 [ 0.54464287]
 [ 0.58035713]
 [ 0.48214287]
 [ 0.60714287]
 [ 0.83928573]
 [ 0.60714287]
 [ 0.6964286 ]
 [ 0.66964287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.5327068 ]
 [ 0.49369961]
 [ 0.53863418]
 [ 0.51505834]
 [ 0.53186458]
 [ 0.57593977]
 [ 0.6509189 ]
 [ 0.60190225]
 [ 0.65773237]
 [ 0.69538569]]
测试集批次数据的前10个数据
 [[ 0.5       ]
 [ 0.4642857 ]
 [ 0.64285713]
 [ 0.44642857]
 [ 0.5714286 ]
 [ 0.16964285]
 [ 0.2767857 ]
 [ 0.5535714 ]
 [ 0.53571427]
 [ 0.44642857]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.3882927 ]
 [ 0.53064537]
 [ 0.60951173]
 [ 0.52946448]
 [ 0.54232413]
 [ 0.58715343]
 [ 0.47600752]
 [ 0.46687815]
 [ 0.53664827]
 [ 0.52063972]]
Generation 1100: train Loss = 0.10532
Generation 1100: test Loss = 0.08976
  2%|         | 1150/50002 [21:10<18:46:10,  1.38s/it]Generation 1150: train Loss = 0.09905
Generation 1150: test Loss = 0.09721
  2%|         | 1200/50002 [22:05<18:37:35,  1.37s/it]训练集批次数据的前10个数据
 [[ 0.375     ]
 [ 0.59821427]
 [ 0.4375    ]
 [ 0.79464287]
 [ 0.16964285]
 [ 0.72321427]
 [ 0.54464287]
 [ 0.51785713]
 [ 0.66071427]
 [ 0.625     ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.40956712]
 [ 0.63357139]
 [ 0.3267701 ]
 [ 0.68111122]
 [ 0.58686376]
 [ 0.56953359]
 [ 0.56818044]
 [ 0.49741003]
 [ 0.5810495 ]
 [ 0.6033951 ]]
测试集批次数据的前10个数据
 [[ 0.51785713]
 [ 0.53571427]
 [ 0.52678573]
 [ 0.45535713]
 [ 0.5       ]
 [ 0.64285713]
 [ 0.51785713]
 [ 0.4732143 ]
 [ 0.54464287]
 [ 0.66964287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.41992584]
 [ 0.55662543]
 [ 0.54716933]
 [ 0.58655769]
 [ 0.49142063]
 [ 0.58794117]
 [ 0.64916927]
 [ 0.53686678]
 [ 0.60861146]
 [ 0.65269512]]
Generation 1200: train Loss = 0.11003
Generation 1200: test Loss = 0.10142
  2%|         | 1250/50002 [23:00<18:45:35,  1.39s/it]Generation 1250: train Loss = 0.09447
Generation 1250: test Loss = 0.10245
  3%|         | 1300/50002 [23:55<18:55:37,  1.40s/it]训练集批次数据的前10个数据
 [[ 0.15178572]
 [ 0.59821427]
 [ 0.5714286 ]
 [ 0.625     ]
 [ 0.5       ]
 [ 0.4107143 ]
 [ 0.58928573]
 [ 0.39285713]
 [ 0.91964287]
 [ 0.5625    ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.3253566 ]
 [ 0.60119677]
 [ 0.61279058]
 [ 0.53332853]
 [ 0.49572283]
 [ 0.55740821]
 [ 0.4793995 ]
 [ 0.47292942]
 [ 0.61616683]
 [ 0.48616397]]
测试集批次数据的前10个数据
 [[ 0.66071427]
 [ 0.52678573]
 [ 0.6964286 ]
 [ 0.6339286 ]
 [ 0.65178573]
 [ 0.51785713]
 [ 0.49107143]
 [ 0.23214285]
 [ 0.5089286 ]
 [ 0.49107143]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.53157514]
 [ 0.44256449]
 [ 0.62900364]
 [ 0.54450536]
 [ 0.65047061]
 [ 0.5103808 ]
 [ 0.4703044 ]
 [ 0.34997463]
 [ 0.47422576]
 [ 0.61272621]]
Generation 1300: train Loss = 0.10488
Generation 1300: test Loss = 0.10374
  3%|         | 1349/50002 [24:48<14:38:08,  1.08s/it]Generation 1350: train Loss = 0.10004
  3%|         | 1350/50002 [24:50<18:42:57,  1.38s/it]Generation 1350: test Loss = 0.10941
  3%|         | 1400/50002 [25:45<18:39:58,  1.38s/it]训练集批次数据的前10个数据
 [[ 0.6785714 ]
 [ 0.48214287]
 [ 0.58928573]
 [ 0.5089286 ]
 [ 0.5535714 ]
 [ 0.65178573]
 [ 0.4107143 ]
 [ 0.26785713]
 [ 0.5535714 ]
 [ 0.58035713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.68679667]
 [ 0.50234628]
 [ 0.55299306]
 [ 0.50237119]
 [ 0.54365695]
 [ 0.57195359]
 [ 0.49406171]
 [ 0.33747411]
 [ 0.65410173]
 [ 0.68005288]]
测试集批次数据的前10个数据
 [[ 0.5089286 ]
 [ 0.5089286 ]
 [ 0.64285713]
 [ 0.6785714 ]
 [ 0.5714286 ]
 [ 0.60714287]
 [ 0.59821427]
 [ 0.35714287]
 [ 0.4642857 ]
 [ 0.66964287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.55064476]
 [ 0.59983522]
 [ 0.47372913]
 [ 0.71923196]
 [ 0.49498668]
 [ 0.4907006 ]
 [ 0.6004045 ]
 [ 0.4327026 ]
 [ 0.56195939]
 [ 0.60806322]]
Generation 1400: train Loss = 0.09958
Generation 1400: test Loss = 0.09544
  3%|         | 1450/50002 [26:39<19:02:55,  1.41s/it]Generation 1450: train Loss = 0.10551
Generation 1450: test Loss = 0.10157
  3%|         | 1499/50002 [27:32<14:30:14,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.5535714 ]
 [ 0.72321427]
 [ 0.41964287]
 [ 0.66964287]
 [ 0.60714287]
 [ 0.64285713]
 [ 0.4375    ]
 [ 0.39285713]
 [ 0.4107143 ]
 [ 0.54464287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.63722694]
 [ 0.48321408]
 [ 0.55494159]
 [ 0.61207998]
 [ 0.60808837]
 [ 0.67385781]
 [ 0.56452274]
 [ 0.56437027]
 [ 0.65141398]
 [ 0.55425304]]
测试集批次数据的前10个数据
 [[ 0.54464287]
 [ 0.79464287]
 [ 0.60714287]
 [ 0.6875    ]
 [ 0.58928573]
 [ 0.8035714 ]
 [ 0.54464287]
 [ 0.65178573]
 [ 0.6875    ]
 [ 0.5535714 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.52661937]
 [ 0.65281785]
 [ 0.62549567]
 [ 0.57659221]
 [ 0.51860917]
 [ 0.64755309]
 [ 0.52575457]
 [ 0.50437611]
 [ 0.57051098]
 [ 0.6256507 ]]
Generation 1500: train Loss = 0.10093
Generation 1500: test Loss = 0.09911
  3%|         | 1549/50002 [28:27<14:27:22,  1.07s/it]Generation 1550: train Loss = 0.10566
Generation 1550: test Loss = 0.09843
  3%|         | 1600/50002 [29:24<18:45:54,  1.40s/it]训练集批次数据的前10个数据
 [[ 0.4375    ]
 [ 0.4375    ]
 [ 0.2767857 ]
 [ 0.6339286 ]
 [ 0.5625    ]
 [ 0.20535715]
 [ 0.4732143 ]
 [ 0.5625    ]
 [ 0.44642857]
 [ 0.60714287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.55786645]
 [ 0.51741672]
 [ 0.37862664]
 [ 0.56812936]
 [ 0.55693138]
 [ 0.47104117]
 [ 0.43033797]
 [ 0.45502311]
 [ 0.42439371]
 [ 0.55800056]]
测试集批次数据的前10个数据
 [[ 0.38392857]
 [ 0.6160714 ]
 [ 0.5       ]
 [ 0.35714287]
 [ 0.5       ]
 [ 0.5714286 ]
 [ 0.4375    ]
 [ 0.48214287]
 [ 0.39285713]
 [ 0.4375    ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.50217998]
 [ 0.55153555]
 [ 0.53307235]
 [ 0.36086455]
 [ 0.56445813]
 [ 0.5574311 ]
 [ 0.50098217]
 [ 0.46851134]
 [ 0.58917159]
 [ 0.48330152]]
Generation 1600: train Loss = 0.10060
Generation 1600: test Loss = 0.10050
  3%|         | 1650/50002 [30:19<18:43:43,  1.39s/it]Generation 1650: train Loss = 0.09501
Generation 1650: test Loss = 0.09251
  3%|         | 1700/50002 [31:13<18:50:43,  1.40s/it]训练集批次数据的前10个数据
 [[ 0.6160714 ]
 [ 0.375     ]
 [ 0.5535714 ]
 [ 0.64285713]
 [ 0.65178573]
 [ 0.66071427]
 [ 0.6160714 ]
 [ 0.54464287]
 [ 0.6964286 ]
 [ 0.5625    ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.55217284]
 [ 0.4968034 ]
 [ 0.73051786]
 [ 0.52210248]
 [ 0.60209072]
 [ 0.59409118]
 [ 0.55516332]
 [ 0.62961131]
 [ 0.6536535 ]
 [ 0.61412102]]
测试集批次数据的前10个数据
 [[ 0.5625    ]
 [ 0.5714286 ]
 [ 0.3392857 ]
 [ 0.48214287]
 [ 0.6875    ]
 [ 0.5625    ]
 [ 0.24107143]
 [ 0.54464287]
 [ 0.33035713]
 [ 0.5       ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.61337924]
 [ 0.55888295]
 [ 0.43193251]
 [ 0.36274445]
 [ 0.6123389 ]
 [ 0.66063142]
 [ 0.53178054]
 [ 0.49463022]
 [ 0.37108946]
 [ 0.4083727 ]]
Generation 1700: train Loss = 0.10317
Generation 1700: test Loss = 0.09904
  3%|         | 1749/50002 [32:06<14:30:54,  1.08s/it]Generation 1750: train Loss = 0.10474
  3%|         | 1750/50002 [32:08<19:07:09,  1.43s/it]Generation 1750: test Loss = 0.11167
  4%|         | 1799/50002 [33:01<14:23:25,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.65178573]
 [ 0.38392857]
 [ 0.44642857]
 [ 0.60714287]
 [ 0.8125    ]
 [ 0.7589286 ]
 [ 0.42857143]
 [ 0.5535714 ]
 [ 0.54464287]
 [ 0.53571427]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.59563887]
 [ 0.55320734]
 [ 0.34753439]
 [ 0.65774739]
 [ 0.66076922]
 [ 0.68915009]
 [ 0.53680927]
 [ 0.64588749]
 [ 0.54350805]
 [ 0.55612582]]
测试集批次数据的前10个数据
 [[ 0.4375    ]
 [ 0.58035713]
 [ 0.42857143]
 [ 0.6785714 ]
 [ 0.35714287]
 [ 0.49107143]
 [ 0.66071427]
 [ 0.60714287]
 [ 0.30357143]
 [ 0.4017857 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.58598644]
 [ 0.54263985]
 [ 0.57407188]
 [ 0.6037249 ]
 [ 0.56879699]
 [ 0.57962239]
 [ 0.62969124]
 [ 0.49102688]
 [ 0.33348224]
 [ 0.42134213]]
Generation 1800: train Loss = 0.09161
Generation 1800: test Loss = 0.11126
  4%|         | 1849/50002 [33:56<14:25:18,  1.08s/it]Generation 1850: train Loss = 0.10213
Generation 1850: test Loss = 0.10544
  4%|         | 1900/50002 [34:53<18:45:20,  1.40s/it]训练集批次数据的前10个数据
 [[ 0.6785714 ]
 [ 0.49107143]
 [ 0.58035713]
 [ 0.36607143]
 [ 0.14285715]
 [ 0.23214285]
 [ 0.48214287]
 [ 0.4732143 ]
 [ 0.51785713]
 [ 0.8214286 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.63005185]
 [ 0.48867834]
 [ 0.54491758]
 [ 0.48888308]
 [ 0.32293528]
 [ 0.47424808]
 [ 0.48553562]
 [ 0.59080601]
 [ 0.48380685]
 [ 0.57290107]]
测试集批次数据的前10个数据
 [[ 0.76785713]
 [ 0.44642857]
 [ 0.23214285]
 [ 0.65178573]
 [ 0.6785714 ]
 [ 0.39285713]
 [ 0.54464287]
 [ 0.30357143]
 [ 0.35714287]
 [ 0.51785713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.6378867 ]
 [ 0.50939053]
 [ 0.33195949]
 [ 0.54144466]
 [ 0.58333635]
 [ 0.59956241]
 [ 0.55946457]
 [ 0.48490596]
 [ 0.39695665]
 [ 0.46015346]]
Generation 1900: train Loss = 0.08713
Generation 1900: test Loss = 0.10464
  4%|         | 1950/50002 [35:48<18:43:24,  1.40s/it]Generation 1950: train Loss = 0.09925
Generation 1950: test Loss = 0.10245
  4%|         | 1999/50002 [36:41<14:22:28,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.71428573]
 [ 0.32142857]
 [ 0.5089286 ]
 [ 0.64285713]
 [ 0.6785714 ]
 [ 0.38392857]
 [ 0.41964287]
 [ 0.3125    ]
 [ 0.625     ]
 [ 0.59821427]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.61606777]
 [ 0.53033721]
 [ 0.58784926]
 [ 0.67742121]
 [ 0.68511879]
 [ 0.32650977]
 [ 0.41175178]
 [ 0.52872312]
 [ 0.61866313]
 [ 0.52604377]]
测试集批次数据的前10个数据
 [[ 0.66071427]
 [ 0.72321427]
 [ 0.14285715]
 [ 0.66964287]
 [ 0.6964286 ]
 [ 0.3482143 ]
 [ 0.66071427]
 [ 0.58035713]
 [ 0.77678573]
 [ 0.76785713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.59921217]
 [ 0.63495678]
 [ 0.43590939]
 [ 0.63866389]
 [ 0.54703593]
 [ 0.43751317]
 [ 0.60949272]
 [ 0.68763959]
 [ 0.68730104]
 [ 0.62951034]]
Generation 2000: train Loss = 0.10088
Generation 2000: test Loss = 0.10114
  4%|         | 2049/50002 [37:37<14:13:38,  1.07s/it]Generation 2050: train Loss = 0.09224
Generation 2050: test Loss = 0.09350
  4%|         | 2100/50002 [38:34<18:49:10,  1.41s/it]训练集批次数据的前10个数据
 [[ 0.52678573]
 [ 0.4642857 ]
 [ 0.17857143]
 [ 0.5714286 ]
 [ 0.45535713]
 [ 0.625     ]
 [ 0.6875    ]
 [ 0.5       ]
 [ 0.64285713]
 [ 0.66964287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.56041217]
 [ 0.5129112 ]
 [ 0.55401248]
 [ 0.59889364]
 [ 0.4110792 ]
 [ 0.4583658 ]
 [ 0.53002709]
 [ 0.5013454 ]
 [ 0.55603206]
 [ 0.60421801]]
测试集批次数据的前10个数据
 [[ 0.6875    ]
 [ 0.44642857]
 [ 0.4642857 ]
 [ 0.76785713]
 [ 0.52678573]
 [ 0.6875    ]
 [ 0.6785714 ]
 [ 0.44642857]
 [ 0.45535713]
 [ 0.5535714 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.47597691]
 [ 0.4785223 ]
 [ 0.5326966 ]
 [ 0.61009085]
 [ 0.43647122]
 [ 0.53002709]
 [ 0.54005951]
 [ 0.54919565]
 [ 0.41960329]
 [ 0.57876992]]
Generation 2100: train Loss = 0.10207
Generation 2100: test Loss = 0.09570
  4%|         | 2149/50002 [39:27<14:18:45,  1.08s/it]Generation 2150: train Loss = 0.10087
Generation 2150: test Loss = 0.09827
  4%|         | 2199/50002 [40:22<14:21:49,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.19642857]
 [ 0.53571427]
 [ 0.4642857 ]
 [ 0.58035713]
 [ 0.375     ]
 [ 0.72321427]
 [ 0.7589286 ]
 [ 0.6875    ]
 [ 0.6785714 ]
 [ 0.375     ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.32524741]
 [ 0.43804038]
 [ 0.39126694]
 [ 0.59480268]
 [ 0.34331709]
 [ 0.59198761]
 [ 0.69748998]
 [ 0.6531558 ]
 [ 0.64593852]
 [ 0.43350238]]
测试集批次数据的前10个数据
 [[ 0.52678573]
 [ 0.33035713]
 [ 0.5714286 ]
 [ 0.53571427]
 [ 0.53571427]
 [ 0.51785713]
 [ 0.3125    ]
 [ 0.41964287]
 [ 0.5625    ]
 [ 0.7410714 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.62086505]
 [ 0.58296365]
 [ 0.60639578]
 [ 0.57624567]
 [ 0.5840776 ]
 [ 0.4382576 ]
 [ 0.31809908]
 [ 0.58199787]
 [ 0.67853516]
 [ 0.64454532]]
Generation 2200: train Loss = 0.10148
Generation 2200: test Loss = 0.09439
  4%|         | 2250/50002 [41:19<18:42:23,  1.41s/it]Generation 2250: train Loss = 0.10021
Generation 2250: test Loss = 0.10214
  5%|         | 2300/50002 [42:14<18:36:25,  1.40s/it]训练集批次数据的前10个数据
 [[ 0.6875    ]
 [ 0.83035713]
 [ 0.66964287]
 [ 0.49107143]
 [ 0.5714286 ]
 [ 0.6964286 ]
 [ 0.39285713]
 [ 0.16964285]
 [ 0.49107143]
 [ 0.17857143]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.51634651]
 [ 0.62476075]
 [ 0.57376772]
 [ 0.63003951]
 [ 0.56930304]
 [ 0.57065547]
 [ 0.4511767 ]
 [ 0.34033856]
 [ 0.56709653]
 [ 0.47880912]]
测试集批次数据的前10个数据
 [[ 0.6785714 ]
 [ 0.51785713]
 [ 0.4732143 ]
 [ 0.3482143 ]
 [ 0.54464287]
 [ 0.5089286 ]
 [ 0.58928573]
 [ 0.22321428]
 [ 0.45535713]
 [ 0.65178573]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.6156404 ]
 [ 0.49862981]
 [ 0.55089039]
 [ 0.47698981]
 [ 0.59627318]
 [ 0.51413012]
 [ 0.48503435]
 [ 0.49226955]
 [ 0.47768283]
 [ 0.56967437]]
Generation 2300: train Loss = 0.09770
Generation 2300: test Loss = 0.10093
  5%|         | 2349/50002 [43:07<14:15:32,  1.08s/it]Generation 2350: train Loss = 0.10340
Generation 2350: test Loss = 0.10178
  5%|         | 2400/50002 [44:04<18:38:08,  1.41s/it]训练集批次数据的前10个数据
 [[ 0.58035713]
 [ 0.5       ]
 [ 0.48214287]
 [ 0.72321427]
 [ 0.4642857 ]
 [ 0.60714287]
 [ 0.3392857 ]
 [ 0.64285713]
 [ 0.90178573]
 [ 0.66964287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.5155189 ]
 [ 0.54553545]
 [ 0.54626328]
 [ 0.61887783]
 [ 0.5343315 ]
 [ 0.60254407]
 [ 0.4772954 ]
 [ 0.57095492]
 [ 0.68091583]
 [ 0.57911229]]
测试集批次数据的前10个数据
 [[ 0.75      ]
 [ 0.49107143]
 [ 0.49107143]
 [ 0.4375    ]
 [ 0.4642857 ]
 [ 0.42857143]
 [ 0.66071427]
 [ 0.6785714 ]
 [ 0.59821427]
 [ 0.49107143]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.72247493]
 [ 0.33491713]
 [ 0.4394598 ]
 [ 0.46683151]
 [ 0.48081526]
 [ 0.45395648]
 [ 0.63000757]
 [ 0.60359704]
 [ 0.62337518]
 [ 0.54537302]]
Generation 2400: train Loss = 0.10269
Generation 2400: test Loss = 0.10372
  5%|         | 2449/50002 [44:57<14:21:03,  1.09s/it]Generation 2450: train Loss = 0.09408
Generation 2450: test Loss = 0.09995
  5%|         | 2500/50002 [45:54<18:38:41,  1.41s/it]训练集批次数据的前10个数据
 [[ 0.77678573]
 [ 0.4017857 ]
 [ 0.6785714 ]
 [ 0.52678573]
 [ 0.4375    ]
 [ 0.3482143 ]
 [ 0.4375    ]
 [ 0.73214287]
 [ 0.4017857 ]
 [ 0.58928573]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.65789902]
 [ 0.4172191 ]
 [ 0.61630714]
 [ 0.52983385]
 [ 0.45843947]
 [ 0.31724581]
 [ 0.51221895]
 [ 0.63109756]
 [ 0.56486142]
 [ 0.55975121]]
测试集批次数据的前10个数据
 [[ 0.54464287]
 [ 0.66964287]
 [ 0.60714287]
 [ 0.42857143]
 [ 0.52678573]
 [ 0.54464287]
 [ 0.51785713]
 [ 0.33035713]
 [ 0.625     ]
 [ 0.5714286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.5996058 ]
 [ 0.58905786]
 [ 0.5283311 ]
 [ 0.45644355]
 [ 0.54967284]
 [ 0.62937641]
 [ 0.52421743]
 [ 0.42391413]
 [ 0.66342282]
 [ 0.55751771]]
Generation 2500: train Loss = 0.09513
Generation 2500: test Loss = 0.09802
  5%|         | 2549/50002 [46:47<14:08:58,  1.07s/it]Generation 2550: train Loss = 0.09944
Generation 2550: test Loss = 0.10038
  5%|         | 2600/50002 [47:44<18:40:28,  1.42s/it]训练集批次数据的前10个数据
 [[ 0.5       ]
 [ 0.5089286 ]
 [ 0.52678573]
 [ 0.6160714 ]
 [ 0.3392857 ]
 [ 0.38392857]
 [ 0.41964287]
 [ 0.58035713]
 [ 0.41964287]
 [ 0.4375    ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.67260134]
 [ 0.5620966 ]
 [ 0.52482462]
 [ 0.61628914]
 [ 0.41655716]
 [ 0.32525867]
 [ 0.55080342]
 [ 0.65672868]
 [ 0.43559492]
 [ 0.38911304]]
测试集批次数据的前10个数据
 [[ 0.4017857 ]
 [ 0.5       ]
 [ 0.8035714 ]
 [ 0.6160714 ]
 [ 0.5       ]
 [ 0.4732143 ]
 [ 0.6785714 ]
 [ 0.5       ]
 [ 0.42857143]
 [ 0.58035713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.49528223]
 [ 0.62371296]
 [ 0.67536044]
 [ 0.5242272 ]
 [ 0.53681552]
 [ 0.55977309]
 [ 0.61375892]
 [ 0.50086451]
 [ 0.46349323]
 [ 0.50435591]]
Generation 2600: train Loss = 0.10309
Generation 2600: test Loss = 0.09967
  5%|         | 2649/50002 [48:37<14:10:53,  1.08s/it]Generation 2650: train Loss = 0.10107
Generation 2650: test Loss = 0.09983
  5%|         | 2699/50002 [49:32<14:02:10,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.64285713]
 [ 0.64285713]
 [ 0.33035713]
 [ 0.77678573]
 [ 0.38392857]
 [ 0.3392857 ]
 [ 0.5       ]
 [ 0.42857143]
 [ 0.52678573]
 [ 0.53571427]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.53715265]
 [ 0.52788275]
 [ 0.33257475]
 [ 0.62125111]
 [ 0.68218851]
 [ 0.47750473]
 [ 0.46287102]
 [ 0.4636865 ]
 [ 0.43342966]
 [ 0.51224655]]
测试集批次数据的前10个数据
 [[ 0.51785713]
 [ 0.48214287]
 [ 0.66071427]
 [ 0.5089286 ]
 [ 0.4642857 ]
 [ 0.23214285]
 [ 0.71428573]
 [ 0.66071427]
 [ 0.6964286 ]
 [ 0.5089286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.51612705]
 [ 0.59148532]
 [ 0.59897387]
 [ 0.65361547]
 [ 0.54325688]
 [ 0.47571069]
 [ 0.57349283]
 [ 0.63225698]
 [ 0.56818944]
 [ 0.63621926]]
Generation 2700: train Loss = 0.09691
Generation 2700: test Loss = 0.10124
  5%|         | 2750/50002 [50:29<18:46:03,  1.43s/it]Generation 2750: train Loss = 0.09410
Generation 2750: test Loss = 0.09628
  6%|         | 2800/50002 [51:24<18:36:42,  1.42s/it]训练集批次数据的前10个数据
 [[ 0.5535714 ]
 [ 0.5714286 ]
 [ 0.54464287]
 [ 0.52678573]
 [ 0.58035713]
 [ 0.625     ]
 [ 0.5       ]
 [ 0.625     ]
 [ 0.60714287]
 [ 0.16071428]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.58745629]
 [ 0.47308797]
 [ 0.65792054]
 [ 0.50791198]
 [ 0.55873811]
 [ 0.64626026]
 [ 0.5623579 ]
 [ 0.60398567]
 [ 0.57939422]
 [ 0.48147863]]
测试集批次数据的前10个数据
 [[ 0.52678573]
 [ 0.6339286 ]
 [ 0.33035713]
 [ 0.4642857 ]
 [ 0.49107143]
 [ 0.49107143]
 [ 0.71428573]
 [ 0.25      ]
 [ 0.5625    ]
 [ 0.3125    ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.53504443]
 [ 0.60305417]
 [ 0.33695427]
 [ 0.61187303]
 [ 0.50541228]
 [ 0.64284015]
 [ 0.70948792]
 [ 0.45889497]
 [ 0.51806384]
 [ 0.32689273]]
Generation 2800: train Loss = 0.08835
Generation 2800: test Loss = 0.10187
  6%|         | 2850/50002 [52:19<18:31:23,  1.41s/it]Generation 2850: train Loss = 0.10399
Generation 2850: test Loss = 0.10025
  6%|         | 2899/50002 [53:12<14:07:18,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.22321428]
 [ 0.4732143 ]
 [ 0.6785714 ]
 [ 0.73214287]
 [ 0.5089286 ]
 [ 0.52678573]
 [ 0.6160714 ]
 [ 0.22321428]
 [ 0.45535713]
 [ 0.5089286 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.35312808]
 [ 0.42381358]
 [ 0.57420373]
 [ 0.50256908]
 [ 0.57255358]
 [ 0.5395816 ]
 [ 0.48631626]
 [ 0.35408249]
 [ 0.36397049]
 [ 0.53931296]]
测试集批次数据的前10个数据
 [[ 0.53571427]
 [ 0.4732143 ]
 [ 0.5535714 ]
 [ 0.5625    ]
 [ 0.64285713]
 [ 0.72321427]
 [ 0.5714286 ]
 [ 0.64285713]
 [ 0.48214287]
 [ 0.45535713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.55673897]
 [ 0.5300014 ]
 [ 0.49841976]
 [ 0.48185241]
 [ 0.60130858]
 [ 0.63954848]
 [ 0.54334426]
 [ 0.53085053]
 [ 0.57779914]
 [ 0.60530823]]
Generation 2900: train Loss = 0.09913
Generation 2900: test Loss = 0.09619
  6%|         | 2950/50002 [54:09<18:38:07,  1.43s/it]Generation 2950: train Loss = 0.10673
Generation 2950: test Loss = 0.10070
  6%|         | 3000/50002 [55:04<18:29:54,  1.42s/it]训练集批次数据的前10个数据
 [[ 0.33035713]
 [ 0.3392857 ]
 [ 0.6964286 ]
 [ 0.53571427]
 [ 0.65178573]
 [ 0.5625    ]
 [ 0.42857143]
 [ 0.42857143]
 [ 0.54464287]
 [ 0.5089286 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.40145165]
 [ 0.37576473]
 [ 0.58578187]
 [ 0.60809684]
 [ 0.59663939]
 [ 0.57374573]
 [ 0.51376557]
 [ 0.4265857 ]
 [ 0.42959166]
 [ 0.53635138]]
测试集批次数据的前10个数据
 [[ 0.16964285]
 [ 0.4375    ]
 [ 0.52678573]
 [ 0.58035713]
 [ 0.66071427]
 [ 0.70535713]
 [ 0.5714286 ]
 [ 0.5535714 ]
 [ 0.6785714 ]
 [ 0.6339286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.5891307 ]
 [ 0.34110457]
 [ 0.59331548]
 [ 0.48889503]
 [ 0.54699647]
 [ 0.66437042]
 [ 0.58768475]
 [ 0.50489193]
 [ 0.66697323]
 [ 0.45941716]]
Generation 3000: train Loss = 0.09480
Generation 3000: test Loss = 0.10468
  6%|         | 3049/50002 [55:57<14:05:32,  1.08s/it]Generation 3050: train Loss = 0.09651
Generation 3050: test Loss = 0.09086
  6%|         | 3100/50002 [56:54<18:30:17,  1.42s/it]训练集批次数据的前10个数据
 [[ 0.6160714 ]
 [ 0.4732143 ]
 [ 0.5       ]
 [ 0.79464287]
 [ 0.59821427]
 [ 0.5535714 ]
 [ 0.5535714 ]
 [ 0.125     ]
 [ 0.4375    ]
 [ 0.4732143 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.60467941]
 [ 0.50667512]
 [ 0.61338127]
 [ 0.69535393]
 [ 0.53362435]
 [ 0.5558244 ]
 [ 0.55437899]
 [ 0.40124783]
 [ 0.45130676]
 [ 0.57636178]]
测试集批次数据的前10个数据
 [[ 0.72321427]
 [ 0.65178573]
 [ 0.5714286 ]
 [ 0.59821427]
 [ 0.48214287]
 [ 0.71428573]
 [ 0.54464287]
 [ 0.375     ]
 [ 0.58035713]
 [ 0.65178573]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.48357424]
 [ 0.45700362]
 [ 0.62400675]
 [ 0.62882084]
 [ 0.57834494]
 [ 0.66766912]
 [ 0.54379797]
 [ 0.69651115]
 [ 0.52293229]
 [ 0.37396526]]
Generation 3100: train Loss = 0.09735
Generation 3100: test Loss = 0.09663
  6%|         | 3150/50002 [57:49<18:27:09,  1.42s/it]Generation 3150: train Loss = 0.08670
Generation 3150: test Loss = 0.09390
  6%|         | 3199/50002 [58:42<13:58:31,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.70535713]
 [ 0.4107143 ]
 [ 0.5089286 ]
 [ 0.65178573]
 [ 0.65178573]
 [ 0.58928573]
 [ 0.3392857 ]
 [ 0.5535714 ]
 [ 0.84821427]
 [ 0.73214287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.64047074]
 [ 0.47447133]
 [ 0.4735893 ]
 [ 0.66880625]
 [ 0.63165867]
 [ 0.44651189]
 [ 0.43441811]
 [ 0.4333165 ]
 [ 0.68309736]
 [ 0.606417  ]]
测试集批次数据的前10个数据
 [[ 0.44642857]
 [ 0.5535714 ]
 [ 0.66071427]
 [ 0.6875    ]
 [ 0.66071427]
 [ 0.5535714 ]
 [ 0.54464287]
 [ 0.36607143]
 [ 0.6160714 ]
 [ 0.4375    ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.57105613]
 [ 0.47848541]
 [ 0.60966384]
 [ 0.63468444]
 [ 0.66937578]
 [ 0.64138985]
 [ 0.56387025]
 [ 0.37601966]
 [ 0.60280502]
 [ 0.46645069]]
Generation 3200: train Loss = 0.09042
Generation 3200: test Loss = 0.09614
  6%|         | 3250/50002 [59:39<18:34:22,  1.43s/it]Generation 3250: train Loss = 0.10144
Generation 3250: test Loss = 0.10340
  7%|         | 3299/50002 [1:00:31<14:01:45,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.54464287]
 [ 0.3392857 ]
 [ 0.5535714 ]
 [ 0.5714286 ]
 [ 0.5       ]
 [ 0.5089286 ]
 [ 0.5       ]
 [ 0.60714287]
 [ 0.8125    ]
 [ 0.625     ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.50974613]
 [ 0.55978143]
 [ 0.53955305]
 [ 0.69280124]
 [ 0.38733917]
 [ 0.54516077]
 [ 0.58084524]
 [ 0.55949003]
 [ 0.57166958]
 [ 0.63506168]]
测试集批次数据的前10个数据
 [[ 0.4107143 ]
 [ 0.66071427]
 [ 0.39285713]
 [ 0.6160714 ]
 [ 0.66071427]
 [ 0.64285713]
 [ 0.49107143]
 [ 0.19642857]
 [ 0.6785714 ]
 [ 0.66964287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.63671291]
 [ 0.53129327]
 [ 0.41783789]
 [ 0.60539973]
 [ 0.56690949]
 [ 0.63795638]
 [ 0.57586926]
 [ 0.3387939 ]
 [ 0.63336265]
 [ 0.56210756]]
Generation 3300: train Loss = 0.10442
Generation 3300: test Loss = 0.10719
  7%|         | 3350/50002 [1:01:29<18:33:30,  1.43s/it]Generation 3350: train Loss = 0.10173
Generation 3350: test Loss = 0.10331
  7%|         | 3399/50002 [1:02:22<13:53:55,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.45535713]
 [ 0.38392857]
 [ 0.6339286 ]
 [ 0.6160714 ]
 [ 0.36607143]
 [ 0.66071427]
 [ 0.48214287]
 [ 0.24107143]
 [ 0.5625    ]
 [ 0.53571427]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.47806162]
 [ 0.46890837]
 [ 0.58760363]
 [ 0.48463061]
 [ 0.54090726]
 [ 0.58096159]
 [ 0.44495273]
 [ 0.33916658]
 [ 0.63029492]
 [ 0.61044014]]
测试集批次数据的前10个数据
 [[ 0.44642857]
 [ 0.5535714 ]
 [ 0.49107143]
 [ 0.65178573]
 [ 0.48214287]
 [ 0.5625    ]
 [ 0.42857143]
 [ 0.5714286 ]
 [ 0.625     ]
 [ 0.70535713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.46154943]
 [ 0.50093424]
 [ 0.60202748]
 [ 0.61159503]
 [ 0.47058159]
 [ 0.55715054]
 [ 0.49131268]
 [ 0.50214934]
 [ 0.57970941]
 [ 0.58971447]]
Generation 3400: train Loss = 0.09593
Generation 3400: test Loss = 0.10797
  7%|         | 3450/50002 [1:03:19<18:22:16,  1.42s/it]Generation 3450: train Loss = 0.10614
Generation 3450: test Loss = 0.10210
  7%|         | 3500/50002 [1:04:14<18:27:09,  1.43s/it]训练集批次数据的前10个数据
 [[ 0.36607143]
 [ 0.59821427]
 [ 0.4732143 ]
 [ 0.32142857]
 [ 0.60714287]
 [ 0.5714286 ]
 [ 0.33035713]
 [ 0.4375    ]
 [ 0.65178573]
 [ 0.3482143 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.37595698]
 [ 0.5205487 ]
 [ 0.3386533 ]
 [ 0.32734933]
 [ 0.72859478]
 [ 0.52054435]
 [ 0.37967634]
 [ 0.5314821 ]
 [ 0.65204954]
 [ 0.43549964]]
测试集批次数据的前10个数据
 [[ 0.5535714 ]
 [ 0.5625    ]
 [ 0.51785713]
 [ 0.49107143]
 [ 0.58928573]
 [ 0.5089286 ]
 [ 0.3125    ]
 [ 0.25      ]
 [ 0.5714286 ]
 [ 0.25      ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.60002136]
 [ 0.52788091]
 [ 0.50721925]
 [ 0.4883523 ]
 [ 0.54767442]
 [ 0.57407618]
 [ 0.33731297]
 [ 0.36183926]
 [ 0.60535014]
 [ 0.42374563]]
Generation 3500: train Loss = 0.10181
Generation 3500: test Loss = 0.09315
  7%|         | 3549/50002 [1:05:06<13:53:57,  1.08s/it]Generation 3550: train Loss = 0.10741
Generation 3550: test Loss = 0.09649
  7%|         | 3599/50002 [1:06:02<13:55:02,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.4732143 ]
 [ 0.44642857]
 [ 0.36607143]
 [ 0.5625    ]
 [ 0.52678573]
 [ 0.60714287]
 [ 0.58035713]
 [ 0.44642857]
 [ 0.4642857 ]
 [ 0.36607143]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.5386939 ]
 [ 0.55285501]
 [ 0.50483871]
 [ 0.46726894]
 [ 0.56017292]
 [ 0.53902471]
 [ 0.55096376]
 [ 0.44219753]
 [ 0.493691  ]
 [ 0.42404628]]
测试集批次数据的前10个数据
 [[ 0.64285713]
 [ 0.3482143 ]
 [ 0.59821427]
 [ 0.48214287]
 [ 0.58035713]
 [ 0.59821427]
 [ 0.48214287]
 [ 0.64285713]
 [ 0.4017857 ]
 [ 0.5089286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.64629626]
 [ 0.45339394]
 [ 0.65966451]
 [ 0.52080321]
 [ 0.49141502]
 [ 0.48739845]
 [ 0.60133243]
 [ 0.54857123]
 [ 0.48107514]
 [ 0.49519545]]
Generation 3600: train Loss = 0.10308
Generation 3600: test Loss = 0.09820
  7%|         | 3650/50002 [1:06:59<19:04:13,  1.48s/it]Generation 3650: train Loss = 0.09814
Generation 3650: test Loss = 0.10725
  7%|         | 3700/50002 [1:07:54<18:28:04,  1.44s/it]训练集批次数据的前10个数据
 [[ 0.78571427]
 [ 0.5535714 ]
 [ 0.6964286 ]
 [ 0.91964287]
 [ 0.53571427]
 [ 0.4642857 ]
 [ 0.3482143 ]
 [ 0.5089286 ]
 [ 0.45535713]
 [ 0.52678573]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.52891207]
 [ 0.56770688]
 [ 0.61072326]
 [ 0.60997021]
 [ 0.45860764]
 [ 0.53936166]
 [ 0.42371765]
 [ 0.69415241]
 [ 0.36379147]
 [ 0.52170753]]
测试集批次数据的前10个数据
 [[ 0.48214287]
 [ 0.6875    ]
 [ 0.48214287]
 [ 0.48214287]
 [ 0.60714287]
 [ 0.58928573]
 [ 0.4642857 ]
 [ 0.5535714 ]
 [ 0.3482143 ]
 [ 0.39285713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.48804387]
 [ 0.57146013]
 [ 0.38359216]
 [ 0.48145562]
 [ 0.61327803]
 [ 0.48798391]
 [ 0.55506164]
 [ 0.59388441]
 [ 0.51519418]
 [ 0.35912836]]
Generation 3700: train Loss = 0.10512
Generation 3700: test Loss = 0.09460
  7%|         | 3750/50002 [1:08:49<18:24:59,  1.43s/it]Generation 3750: train Loss = 0.09454
Generation 3750: test Loss = 0.09260
  8%|         | 3799/50002 [1:09:42<13:47:43,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.5714286 ]
 [ 0.4732143 ]
 [ 0.70535713]
 [ 0.4107143 ]
 [ 0.6339286 ]
 [ 0.58928573]
 [ 0.52678573]
 [ 0.45535713]
 [ 0.6339286 ]
 [ 0.58035713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.65021187]
 [ 0.5207094 ]
 [ 0.71722174]
 [ 0.38773605]
 [ 0.5814141 ]
 [ 0.60925341]
 [ 0.45095012]
 [ 0.46320909]
 [ 0.58549207]
 [ 0.4978956 ]]
测试集批次数据的前10个数据
 [[ 0.48214287]
 [ 0.2857143 ]
 [ 0.625     ]
 [ 0.6785714 ]
 [ 0.22321428]
 [ 0.17857143]
 [ 0.71428573]
 [ 0.58035713]
 [ 0.58035713]
 [ 0.48214287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.60833764]
 [ 0.40709272]
 [ 0.52047288]
 [ 0.67201972]
 [ 0.49254102]
 [ 0.33206978]
 [ 0.6422447 ]
 [ 0.51723623]
 [ 0.59449774]
 [ 0.46070862]]
Generation 3800: train Loss = 0.09704
Generation 3800: test Loss = 0.09997
  8%|         | 3850/50002 [1:10:39<18:24:38,  1.44s/it]Generation 3850: train Loss = 0.10415
Generation 3850: test Loss = 0.09813
  8%|         | 3899/50002 [1:11:32<13:44:47,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.30357143]
 [ 0.64285713]
 [ 0.52678573]
 [ 0.58928573]
 [ 0.5714286 ]
 [ 0.4017857 ]
 [ 0.6339286 ]
 [ 0.8125    ]
 [ 0.5535714 ]
 [ 0.58035713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.32756299]
 [ 0.54784918]
 [ 0.50029767]
 [ 0.540748  ]
 [ 0.57143909]
 [ 0.59488106]
 [ 0.53168792]
 [ 0.66255867]
 [ 0.5124867 ]
 [ 0.4993355 ]]
测试集批次数据的前10个数据
 [[ 0.4107143 ]
 [ 0.5535714 ]
 [ 0.5714286 ]
 [ 0.52678573]
 [ 0.2767857 ]
 [ 0.59821427]
 [ 0.3392857 ]
 [ 0.70535713]
 [ 0.35714287]
 [ 0.66964287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.49459466]
 [ 0.58768594]
 [ 0.60840571]
 [ 0.42887509]
 [ 0.48477635]
 [ 0.48925057]
 [ 0.52301997]
 [ 0.61635745]
 [ 0.32635009]
 [ 0.66526246]]
Generation 3900: train Loss = 0.09789
Generation 3900: test Loss = 0.10882
  8%|         | 3949/50002 [1:12:27<13:42:12,  1.07s/it]Generation 3950: train Loss = 0.10532
Generation 3950: test Loss = 0.10300
  8%|         | 3999/50002 [1:13:22<13:42:32,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.6964286 ]
 [ 0.48214287]
 [ 0.6339286 ]
 [ 0.35714287]
 [ 0.44642857]
 [ 0.60714287]
 [ 0.58035713]
 [ 0.41964287]
 [ 0.4375    ]
 [ 0.42857143]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.65095878]
 [ 0.50020963]
 [ 0.45302409]
 [ 0.43586159]
 [ 0.36923251]
 [ 0.56880414]
 [ 0.56523824]
 [ 0.52323955]
 [ 0.5667423 ]
 [ 0.47705203]]
测试集批次数据的前10个数据
 [[ 0.75      ]
 [ 0.53571427]
 [ 0.4107143 ]
 [ 0.70535713]
 [ 0.41964287]
 [ 0.58035713]
 [ 0.2857143 ]
 [ 0.4642857 ]
 [ 0.60714287]
 [ 0.5714286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.65872562]
 [ 0.58776629]
 [ 0.56233454]
 [ 0.67916012]
 [ 0.61731088]
 [ 0.51825994]
 [ 0.41422525]
 [ 0.45262039]
 [ 0.52373564]
 [ 0.55825567]]
Generation 4000: train Loss = 0.10934
Generation 4000: test Loss = 0.10151
  8%|         | 4049/50002 [1:14:18<13:41:52,  1.07s/it]Generation 4050: train Loss = 0.09582
Generation 4050: test Loss = 0.10234
  8%|         | 4099/50002 [1:15:12<13:46:38,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.59821427]
 [ 0.15178572]
 [ 0.48214287]
 [ 0.08035714]
 [ 0.5089286 ]
 [ 0.35714287]
 [ 0.6785714 ]
 [ 0.66071427]
 [ 0.65178573]
 [ 0.76785713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.63964093]
 [ 0.48112679]
 [ 0.36270988]
 [ 0.32161403]
 [ 0.67517614]
 [ 0.33615676]
 [ 0.59139073]
 [ 0.4989486 ]
 [ 0.54658622]
 [ 0.65635145]]
测试集批次数据的前10个数据
 [[ 0.5089286 ]
 [ 0.75      ]
 [ 0.49107143]
 [ 0.5625    ]
 [ 0.51785713]
 [ 0.6785714 ]
 [ 0.39285713]
 [ 0.5625    ]
 [ 0.32142857]
 [ 0.6785714 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.47791761]
 [ 0.72243911]
 [ 0.53850693]
 [ 0.54933184]
 [ 0.50660235]
 [ 0.62786686]
 [ 0.59891784]
 [ 0.52603912]
 [ 0.36073825]
 [ 0.53001893]]
Generation 4100: train Loss = 0.10946
Generation 4100: test Loss = 0.10051
  8%|         | 4150/50002 [1:16:10<18:19:22,  1.44s/it]Generation 4150: train Loss = 0.09891
Generation 4150: test Loss = 0.09564
  8%|         | 4199/50002 [1:17:03<13:44:38,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.52678573]
 [ 0.58928573]
 [ 0.5089286 ]
 [ 0.5535714 ]
 [ 0.48214287]
 [ 0.71428573]
 [ 0.60714287]
 [ 0.48214287]
 [ 0.625     ]
 [ 0.5535714 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.51254499]
 [ 0.54453593]
 [ 0.44522905]
 [ 0.51598543]
 [ 0.50941467]
 [ 0.63223779]
 [ 0.6610353 ]
 [ 0.51516759]
 [ 0.56625319]
 [ 0.48851696]]
测试集批次数据的前10个数据
 [[ 0.66964287]
 [ 0.83035713]
 [ 0.54464287]
 [ 0.33035713]
 [ 0.75      ]
 [ 0.59821427]
 [ 0.65178573]
 [ 0.2857143 ]
 [ 0.5625    ]
 [ 0.70535713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.60517895]
 [ 0.69591248]
 [ 0.55304432]
 [ 0.36705488]
 [ 0.63854647]
 [ 0.61098319]
 [ 0.56020468]
 [ 0.42952508]
 [ 0.66571683]
 [ 0.62122941]]
Generation 4200: train Loss = 0.10146
Generation 4200: test Loss = 0.09772
  8%|         | 4249/50002 [1:17:58<13:39:52,  1.08s/it]Generation 4250: train Loss = 0.10533
Generation 4250: test Loss = 0.09960
  9%|         | 4300/50002 [1:18:55<18:26:19,  1.45s/it]训练集批次数据的前10个数据
 [[ 0.6785714 ]
 [ 0.85714287]
 [ 0.54464287]
 [ 0.5089286 ]
 [ 0.60714287]
 [ 0.72321427]
 [ 0.77678573]
 [ 0.42857143]
 [ 0.625     ]
 [ 0.25      ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.60374957]
 [ 0.59893364]
 [ 0.5515871 ]
 [ 0.50107634]
 [ 0.6029489 ]
 [ 0.72746706]
 [ 0.69384372]
 [ 0.3505947 ]
 [ 0.62245429]
 [ 0.43046683]]
测试集批次数据的前10个数据
 [[ 0.33035713]
 [ 0.6875    ]
 [ 0.4642857 ]
 [ 0.64285713]
 [ 0.53571427]
 [ 0.45535713]
 [ 0.36607143]
 [ 0.375     ]
 [ 0.51785713]
 [ 0.60714287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.5363369 ]
 [ 0.50893968]
 [ 0.34916782]
 [ 0.64603651]
 [ 0.51636124]
 [ 0.4047744 ]
 [ 0.53848177]
 [ 0.51444268]
 [ 0.50656098]
 [ 0.648588  ]]
Generation 4300: train Loss = 0.09374
Generation 4300: test Loss = 0.09546
  9%|         | 4349/50002 [1:19:48<13:40:41,  1.08s/it]Generation 4350: train Loss = 0.09822
  9%|         | 4350/50002 [1:19:50<18:28:05,  1.46s/it]Generation 4350: test Loss = 0.10830
  9%|         | 4399/50002 [1:20:43<13:36:52,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.7589286 ]
 [ 0.5089286 ]
 [ 0.11607143]
 [ 0.4375    ]
 [ 0.4107143 ]
 [ 0.70535713]
 [ 0.4642857 ]
 [ 0.5       ]
 [ 0.8035714 ]
 [ 0.39285713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.65989172]
 [ 0.55594307]
 [ 0.39077303]
 [ 0.48048413]
 [ 0.55994636]
 [ 0.57964492]
 [ 0.52349079]
 [ 0.43486744]
 [ 0.67210543]
 [ 0.42544973]]
测试集批次数据的前10个数据
 [[ 0.45535713]
 [ 0.375     ]
 [ 0.53571427]
 [ 0.64285713]
 [ 0.36607143]
 [ 0.49107143]
 [ 0.66964287]
 [ 0.08928572]
 [ 0.58035713]
 [ 0.49107143]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.56934786]
 [ 0.4383558 ]
 [ 0.57967269]
 [ 0.61017817]
 [ 0.47766268]
 [ 0.48184398]
 [ 0.63647783]
 [ 0.44964036]
 [ 0.56727898]
 [ 0.46989596]]
Generation 4400: train Loss = 0.09851
Generation 4400: test Loss = 0.09245
  9%|         | 4449/50002 [1:21:38<13:39:45,  1.08s/it]Generation 4450: train Loss = 0.09379
Generation 4450: test Loss = 0.09984
  9%|         | 4499/50002 [1:22:33<13:38:14,  1.08s/it]训练集批次数据的前10个数据
  9%|         | 4500/50002 [1:22:36<18:21:39,  1.45s/it] [[ 0.59821427]
 [ 0.4732143 ]
 [ 0.8125    ]
 [ 0.45535713]
 [ 0.5089286 ]
 [ 0.5535714 ]
 [ 0.51785713]
 [ 0.45535713]
 [ 0.52678573]
 [ 0.51785713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.52047384]
 [ 0.58910513]
 [ 0.68819791]
 [ 0.41950256]
 [ 0.52464479]
 [ 0.52823615]
 [ 0.3798658 ]
 [ 0.53593218]
 [ 0.54598916]
 [ 0.54113293]]
测试集批次数据的前10个数据
 [[ 0.75      ]
 [ 0.52678573]
 [ 0.6785714 ]
 [ 0.5       ]
 [ 0.49107143]
 [ 0.66071427]
 [ 0.38392857]
 [ 0.8035714 ]
 [ 0.71428573]
 [ 0.75      ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.65899265]
 [ 0.5393768 ]
 [ 0.65739357]
 [ 0.53273356]
 [ 0.52193248]
 [ 0.62996358]
 [ 0.48760903]
 [ 0.63694894]
 [ 0.63556445]
 [ 0.60238481]]
Generation 4500: train Loss = 0.10493
Generation 4500: test Loss = 0.09870
  9%|         | 4550/50002 [1:23:31<18:14:52,  1.45s/it]Generation 4550: train Loss = 0.10329
Generation 4550: test Loss = 0.09859
  9%|         | 4600/50002 [1:24:26<18:17:41,  1.45s/it]训练集批次数据的前10个数据
 [[ 0.5089286 ]
 [ 0.64285713]
 [ 0.52678573]
 [ 0.51785713]
 [ 0.5535714 ]
 [ 0.5714286 ]
 [ 0.45535713]
 [ 0.625     ]
 [ 0.65178573]
 [ 0.65178573]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.48179859]
 [ 0.66459   ]
 [ 0.57086551]
 [ 0.53672647]
 [ 0.54271346]
 [ 0.55464315]
 [ 0.49233997]
 [ 0.52338439]
 [ 0.60032976]
 [ 0.54681915]]
测试集批次数据的前10个数据
 [[ 0.64285713]
 [ 0.64285713]
 [ 0.4642857 ]
 [ 0.76785713]
 [ 0.5625    ]
 [ 0.36607143]
 [ 0.5089286 ]
 [ 0.44642857]
 [ 0.5625    ]
 [ 0.7589286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.61388671]
 [ 0.54841495]
 [ 0.62002093]
 [ 0.57240117]
 [ 0.43849108]
 [ 0.49206012]
 [ 0.54375124]
 [ 0.55828106]
 [ 0.59195435]
 [ 0.60858989]]
Generation 4600: train Loss = 0.09480
Generation 4600: test Loss = 0.10850
  9%|         | 4649/50002 [1:25:19<13:36:18,  1.08s/it]Generation 4650: train Loss = 0.10128
  9%|         | 4650/50002 [1:25:21<18:19:28,  1.45s/it]Generation 4650: test Loss = 0.10245
  9%|         | 4700/50002 [1:26:16<18:27:39,  1.47s/it]训练集批次数据的前10个数据
 [[ 0.54464287]
 [ 0.58928573]
 [ 0.6964286 ]
 [ 0.60714287]
 [ 0.42857143]
 [ 0.5       ]
 [ 0.6785714 ]
 [ 0.4107143 ]
 [ 0.58928573]
 [ 0.7410714 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.56868398]
 [ 0.57975334]
 [ 0.62854564]
 [ 0.59438211]
 [ 0.40171266]
 [ 0.63507944]
 [ 0.68316567]
 [ 0.494261  ]
 [ 0.51249045]
 [ 0.63666672]]
测试集批次数据的前10个数据
 [[ 0.39285713]
 [ 0.58035713]
 [ 0.42857143]
 [ 0.51785713]
 [ 0.6785714 ]
 [ 0.51785713]
 [ 0.5625    ]
 [ 0.3482143 ]
 [ 0.5089286 ]
 [ 0.4732143 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.59991455]
 [ 0.50777411]
 [ 0.52655524]
 [ 0.54817367]
 [ 0.60337782]
 [ 0.48435193]
 [ 0.57925272]
 [ 0.52661765]
 [ 0.56296539]
 [ 0.49605659]]
Generation 4700: train Loss = 0.09943
Generation 4700: test Loss = 0.09577
  9%|         | 4750/50002 [1:27:11<18:27:46,  1.47s/it]Generation 4750: train Loss = 0.09813
Generation 4750: test Loss = 0.10157
 10%|         | 4800/50002 [1:28:06<18:18:36,  1.46s/it]训练集批次数据的前10个数据
 [[ 0.41964287]
 [ 0.66071427]
 [ 0.4375    ]
 [ 0.51785713]
 [ 0.7589286 ]
 [ 0.5535714 ]
 [ 0.66071427]
 [ 0.48214287]
 [ 0.5089286 ]
 [ 0.48214287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.58647519]
 [ 0.55743921]
 [ 0.51614076]
 [ 0.45253527]
 [ 0.60241413]
 [ 0.59042966]
 [ 0.64528203]
 [ 0.60799932]
 [ 0.58313233]
 [ 0.49358758]]
测试集批次数据的前10个数据
 [[ 0.49107143]
 [ 0.60714287]
 [ 0.35714287]
 [ 0.35714287]
 [ 0.52678573]
 [ 0.5535714 ]
 [ 0.65178573]
 [ 0.6875    ]
 [ 0.75      ]
 [ 0.2767857 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.47652665]
 [ 0.59103388]
 [ 0.37515333]
 [ 0.34585938]
 [ 0.49580365]
 [ 0.52652562]
 [ 0.5905273 ]
 [ 0.61275697]
 [ 0.59963191]
 [ 0.43303925]]
Generation 4800: train Loss = 0.09056
Generation 4800: test Loss = 0.08733
 10%|         | 4849/50002 [1:28:59<13:25:55,  1.07s/it]Generation 4850: train Loss = 0.10067
Generation 4850: test Loss = 0.09819
 10%|         | 4900/50002 [1:29:56<18:18:02,  1.46s/it]训练集批次数据的前10个数据
 [[ 0.07142857]
 [ 0.66964287]
 [ 0.59821427]
 [ 0.53571427]
 [ 0.52678573]
 [ 0.48214287]
 [ 0.53571427]
 [ 0.5089286 ]
 [ 0.5714286 ]
 [ 0.5714286 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.32521889]
 [ 0.61799991]
 [ 0.61220139]
 [ 0.65710449]
 [ 0.4957372 ]
 [ 0.58192825]
 [ 0.56041932]
 [ 0.5037303 ]
 [ 0.62406003]
 [ 0.54055047]]
测试集批次数据的前10个数据
 [[ 0.51785713]
 [ 0.58928573]
 [ 0.6339286 ]
 [ 0.6339286 ]
 [ 0.53571427]
 [ 0.32142857]
 [ 0.66964287]
 [ 0.59821427]
 [ 0.5       ]
 [ 0.52678573]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.51302844]
 [ 0.55291003]
 [ 0.58150542]
 [ 0.6703409 ]
 [ 0.4625845 ]
 [ 0.47973973]
 [ 0.70215416]
 [ 0.63299161]
 [ 0.56005728]
 [ 0.47279024]]
Generation 4900: train Loss = 0.09985
Generation 4900: test Loss = 0.09662
 10%|         | 4949/50002 [1:30:49<13:33:25,  1.08s/it]Generation 4950: train Loss = 0.10521
Generation 4950: test Loss = 0.10059
 10%|         | 4999/50002 [1:31:44<13:31:24,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.6875    ]
 [ 0.6160714 ]
 [ 0.42857143]
 [ 0.21428572]
 [ 0.48214287]
 [ 0.4732143 ]
 [ 0.48214287]
 [ 0.44642857]
 [ 0.58035713]
 [ 0.48214287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.66406894]
 [ 0.65478766]
 [ 0.46783212]
 [ 0.41099513]
 [ 0.43150741]
 [ 0.5261057 ]
 [ 0.43605417]
 [ 0.47673857]
 [ 0.55756372]
 [ 0.59029698]]
测试集批次数据的前10个数据
 [[ 0.65178573]
 [ 0.39285713]
 [ 0.48214287]
 [ 0.33035713]
 [ 0.32142857]
 [ 0.64285713]
 [ 0.58928573]
 [ 0.5714286 ]
 [ 0.59821427]
 [ 0.5714286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.5642181 ]
 [ 0.59826505]
 [ 0.61722392]
 [ 0.34331042]
 [ 0.5107556 ]
 [ 0.5913192 ]
 [ 0.48450476]
 [ 0.48552901]
 [ 0.52555561]
 [ 0.53458285]]
Generation 5000: train Loss = 0.09448
Generation 5000: test Loss = 0.10557
 10%|         | 5049/50002 [1:32:39<13:30:30,  1.08s/it]Generation 5050: train Loss = 0.09808
Generation 5050: test Loss = 0.10037
 10%|         | 5099/50002 [1:33:34<13:23:00,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.6160714 ]
 [ 0.25892857]
 [ 0.70535713]
 [ 0.7589286 ]
 [ 0.5625    ]
 [ 0.625     ]
 [ 0.5625    ]
 [ 0.6339286 ]
 [ 0.52678573]
 [ 0.35714287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.58307695]
 [ 0.53306991]
 [ 0.64236605]
 [ 0.66059148]
 [ 0.52630842]
 [ 0.55560416]
 [ 0.58024204]
 [ 0.58600235]
 [ 0.51751637]
 [ 0.47769415]]
测试集批次数据的前10个数据
 [[ 0.33035713]
 [ 0.6875    ]
 [ 0.5714286 ]
 [ 0.6160714 ]
 [ 0.23214285]
 [ 0.49107143]
 [ 0.4375    ]
 [ 0.58035713]
 [ 0.6964286 ]
 [ 0.9464286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.50596607]
 [ 0.60483432]
 [ 0.60666901]
 [ 0.58963728]
 [ 0.48071879]
 [ 0.61241913]
 [ 0.34092033]
 [ 0.5042311 ]
 [ 0.55228037]
 [ 0.69131058]]
Generation 5100: train Loss = 0.09590
Generation 5100: test Loss = 0.10514
 10%|         | 5149/50002 [1:34:29<13:24:54,  1.08s/it]Generation 5150: train Loss = 0.10494
Generation 5150: test Loss = 0.09109
 10%|         | 5200/50002 [1:35:27<18:17:45,  1.47s/it]训练集批次数据的前10个数据
 [[ 0.60714287]
 [ 0.6785714 ]
 [ 0.5089286 ]
 [ 0.48214287]
 [ 0.5089286 ]
 [ 0.6964286 ]
 [ 0.54464287]
 [ 0.5625    ]
 [ 0.4732143 ]
 [ 0.5714286 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.46346855]
 [ 0.64496946]
 [ 0.57243443]
 [ 0.45925263]
 [ 0.48322845]
 [ 0.5859701 ]
 [ 0.52612978]
 [ 0.54637295]
 [ 0.42718813]
 [ 0.45344114]]
测试集批次数据的前10个数据
 [[ 0.5535714 ]
 [ 0.4375    ]
 [ 0.70535713]
 [ 0.36607143]
 [ 0.65178573]
 [ 0.52678573]
 [ 0.36607143]
 [ 0.35714287]
 [ 0.49107143]
 [ 0.78571427]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.73033309]
 [ 0.46392989]
 [ 0.55415469]
 [ 0.43046242]
 [ 0.59055704]
 [ 0.46600628]
 [ 0.4112677 ]
 [ 0.39861062]
 [ 0.57544911]
 [ 0.53525352]]
Generation 5200: train Loss = 0.09764
Generation 5200: test Loss = 0.10174
 10%|         | 5250/50002 [1:36:22<18:10:45,  1.46s/it]Generation 5250: train Loss = 0.10148
Generation 5250: test Loss = 0.10362
 11%|         | 5300/50002 [1:37:17<18:18:27,  1.47s/it]训练集批次数据的前10个数据
 [[ 0.5       ]
 [ 0.51785713]
 [ 0.4642857 ]
 [ 0.5089286 ]
 [ 0.49107143]
 [ 0.58035713]
 [ 0.14285715]
 [ 0.7410714 ]
 [ 0.6875    ]
 [ 0.45535713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.52112561]
 [ 0.52130336]
 [ 0.56575894]
 [ 0.47146249]
 [ 0.59248567]
 [ 0.54946375]
 [ 0.43331289]
 [ 0.73353243]
 [ 0.60729808]
 [ 0.48723638]]
测试集批次数据的前10个数据
 [[ 0.4017857 ]
 [ 0.91964287]
 [ 0.60714287]
 [ 0.65178573]
 [ 0.4375    ]
 [ 0.39285713]
 [ 0.4732143 ]
 [ 0.44642857]
 [ 0.4017857 ]
 [ 0.73214287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.42183715]
 [ 0.59240878]
 [ 0.65571344]
 [ 0.54134917]
 [ 0.46346244]
 [ 0.51705354]
 [ 0.58280814]
 [ 0.55289549]
 [ 0.42146423]
 [ 0.68715382]]
Generation 5300: train Loss = 0.09605
Generation 5300: test Loss = 0.10278
 11%|         | 5350/50002 [1:38:12<18:11:27,  1.47s/it]Generation 5350: train Loss = 0.09985
Generation 5350: test Loss = 0.10238
 11%|         | 5399/50002 [1:39:05<13:19:46,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.38392857]
 [ 0.42857143]
 [ 0.4017857 ]
 [ 0.66964287]
 [ 0.29464287]
 [ 0.66071427]
 [ 0.4642857 ]
 [ 0.45535713]
 [ 0.6339286 ]
 [ 0.59821427]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.4187502 ]
 [ 0.54603624]
 [ 0.46878666]
 [ 0.62699723]
 [ 0.49842286]
 [ 0.58222747]
 [ 0.50484449]
 [ 0.41138119]
 [ 0.59799594]
 [ 0.61593139]]
测试集批次数据的前10个数据
 [[ 0.38392857]
 [ 0.7410714 ]
 [ 0.6785714 ]
 [ 0.66964287]
 [ 0.53571427]
 [ 0.5089286 ]
 [ 0.48214287]
 [ 0.64285713]
 [ 0.49107143]
 [ 0.54464287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.37164626]
 [ 0.70920056]
 [ 0.59718376]
 [ 0.61211681]
 [ 0.6314733 ]
 [ 0.51441264]
 [ 0.43972626]
 [ 0.63696557]
 [ 0.61233079]
 [ 0.54876018]]
Generation 5400: train Loss = 0.09601
Generation 5400: test Loss = 0.09819
 11%|         | 5450/50002 [1:40:02<18:09:49,  1.47s/it]Generation 5450: train Loss = 0.08239
Generation 5450: test Loss = 0.09987
 11%|         | 5499/50002 [1:40:55<13:20:37,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.60714287]
 [ 0.4017857 ]
 [ 0.6339286 ]
 [ 0.53571427]
 [ 0.48214287]
 [ 0.48214287]
 [ 0.7410714 ]
 [ 0.59821427]
 [ 0.48214287]
 [ 0.4642857 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.57512701]
 [ 0.61114764]
 [ 0.50406373]
 [ 0.56985462]
 [ 0.50946355]
 [ 0.50283867]
 [ 0.64169252]
 [ 0.56414491]
 [ 0.58214295]
 [ 0.34920186]]
测试集批次数据的前10个数据
 [[ 0.54464287]
 [ 0.73214287]
 [ 0.8660714 ]
 [ 0.39285713]
 [ 0.35714287]
 [ 0.20535715]
 [ 0.38392857]
 [ 0.49107143]
 [ 0.42857143]
 [ 0.54464287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.57810998]
 [ 0.54282433]
 [ 0.65833306]
 [ 0.42483485]
 [ 0.45580342]
 [ 0.50758183]
 [ 0.52727735]
 [ 0.51089561]
 [ 0.56819761]
 [ 0.58710539]]
Generation 5500: train Loss = 0.10760
Generation 5500: test Loss = 0.10875
 11%|         | 5549/50002 [1:41:50<13:19:51,  1.08s/it]Generation 5550: train Loss = 0.08971
Generation 5550: test Loss = 0.09642
 11%|         | 5600/50002 [1:42:48<18:10:04,  1.47s/it]训练集批次数据的前10个数据
 [[ 0.44642857]
 [ 0.65178573]
 [ 0.53571427]
 [ 0.4732143 ]
 [ 0.58035713]
 [ 0.73214287]
 [ 0.38392857]
 [ 0.6785714 ]
 [ 0.53571427]
 [ 0.625     ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.52288222]
 [ 0.50341356]
 [ 0.59243727]
 [ 0.50713879]
 [ 0.60220951]
 [ 0.66109169]
 [ 0.51574802]
 [ 0.5830313 ]
 [ 0.5815503 ]
 [ 0.6032573 ]]
测试集批次数据的前10个数据
 [[ 0.5625    ]
 [ 0.5       ]
 [ 0.71428573]
 [ 0.60714287]
 [ 0.60714287]
 [ 0.48214287]
 [ 0.49107143]
 [ 0.4732143 ]
 [ 0.6964286 ]
 [ 0.60714287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.55269182]
 [ 0.54580605]
 [ 0.59746361]
 [ 0.60613054]
 [ 0.57531118]
 [ 0.50272751]
 [ 0.4562422 ]
 [ 0.50916731]
 [ 0.54575497]
 [ 0.59168661]]
Generation 5600: train Loss = 0.10353
Generation 5600: test Loss = 0.09633
 11%|        | 5649/50002 [1:43:40<13:13:21,  1.07s/it]Generation 5650: train Loss = 0.09659
Generation 5650: test Loss = 0.10026
 11%|        | 5699/50002 [1:44:36<13:13:07,  1.07s/it]训练集批次数据的前10个数据
 [[ 0.58928573]
 [ 0.4732143 ]
 [ 0.2767857 ]
 [ 0.625     ]
 [ 0.53571427]
 [ 0.4375    ]
 [ 0.65178573]
 [ 0.5535714 ]
 [ 0.5535714 ]
 [ 0.35714287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.59463179]
 [ 0.52791512]
 [ 0.32390532]
 [ 0.63257879]
 [ 0.6019513 ]
 [ 0.50438231]
 [ 0.58529127]
 [ 0.60201991]
 [ 0.59824181]
 [ 0.44187999]]
测试集批次数据的前10个数据
 [[ 0.5714286 ]
 [ 0.45535713]
 [ 0.6785714 ]
 [ 0.48214287]
 [ 0.5089286 ]
 [ 0.5       ]
 [ 0.59821427]
 [ 0.45535713]
 [ 0.2857143 ]
 [ 0.39285713]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.58388883]
 [ 0.51163852]
 [ 0.65415585]
 [ 0.53851545]
 [ 0.61619723]
 [ 0.51125681]
 [ 0.49270993]
 [ 0.53218502]
 [ 0.32798636]
 [ 0.34781343]]
Generation 5700: train Loss = 0.09341
Generation 5700: test Loss = 0.09726
 11%|        | 5750/50002 [1:45:34<18:12:05,  1.48s/it]Generation 5750: train Loss = 0.09791
Generation 5750: test Loss = 0.09575
 12%|        | 5799/50002 [1:46:28<13:19:41,  1.09s/it]训练集批次数据的前10个数据
 [[ 0.38392857]
 [ 0.59821427]
 [ 0.66964287]
 [ 0.77678573]
 [ 0.5535714 ]
 [ 0.30357143]
 [ 0.70535713]
 [ 0.59821427]
 [ 0.0625    ]
 [ 0.58035713]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.48140353]
 [ 0.49719223]
 [ 0.54460961]
 [ 0.68971735]
 [ 0.53567249]
 [ 0.58131528]
 [ 0.57477945]
 [ 0.42250961]
 [ 0.42413142]
 [ 0.56503266]]
测试集批次数据的前10个数据
 [[ 0.59821427]
 [ 0.60714287]
 [ 0.29464287]
 [ 0.52678573]
 [ 0.5535714 ]
 [ 0.48214287]
 [ 0.54464287]
 [ 0.53571427]
 [ 0.3482143 ]
 [ 0.2857143 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.62365007]
 [ 0.5882585 ]
 [ 0.48107564]
 [ 0.6200285 ]
 [ 0.60015321]
 [ 0.43305951]
 [ 0.45727664]
 [ 0.47400364]
 [ 0.45652187]
 [ 0.3986007 ]]
Generation 5800: train Loss = 0.10914
Generation 5800: test Loss = 0.10764
 12%|        | 5849/50002 [1:47:25<13:21:44,  1.09s/it]Generation 5850: train Loss = 0.09949
Generation 5850: test Loss = 0.10121
 12%|        | 5900/50002 [1:48:23<18:49:24,  1.54s/it]训练集批次数据的前10个数据
 [[ 0.66964287]
 [ 0.42857143]
 [ 0.70535713]
 [ 0.60714287]
 [ 0.54464287]
 [ 0.48214287]
 [ 0.48214287]
 [ 0.5089286 ]
 [ 0.5535714 ]
 [ 0.65178573]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.58286476]
 [ 0.4492631 ]
 [ 0.43713188]
 [ 0.559313  ]
 [ 0.52934182]
 [ 0.45272782]
 [ 0.50663388]
 [ 0.56620193]
 [ 0.59426492]
 [ 0.55624431]]
测试集批次数据的前10个数据
 [[ 0.2857143 ]
 [ 0.30357143]
 [ 0.4642857 ]
 [ 0.6964286 ]
 [ 0.5       ]
 [ 0.64285713]
 [ 0.4732143 ]
 [ 0.59821427]
 [ 0.48214287]
 [ 0.35714287]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.42915016]
 [ 0.59617621]
 [ 0.55177164]
 [ 0.61026037]
 [ 0.57870388]
 [ 0.69264352]
 [ 0.46746555]
 [ 0.54799062]
 [ 0.5485332 ]
 [ 0.36094072]]
Generation 5900: train Loss = 0.09834
Generation 5900: test Loss = 0.10528
 12%|        | 5950/50002 [1:49:19<18:18:08,  1.50s/it]Generation 5950: train Loss = 0.09311
Generation 5950: test Loss = 0.10642
 12%|        | 5999/50002 [1:50:13<13:14:36,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.72321427]
 [ 0.5714286 ]
 [ 0.54464287]
 [ 0.5089286 ]
 [ 0.65178573]
 [ 0.41964287]
 [ 0.625     ]
 [ 0.625     ]
 [ 0.36607143]
 [ 0.41964287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.65005863]
 [ 0.5098967 ]
 [ 0.59667373]
 [ 0.56709385]
 [ 0.55119538]
 [ 0.33690524]
 [ 0.56617665]
 [ 0.55467826]
 [ 0.55736673]
 [ 0.43534768]]
测试集批次数据的前10个数据
 [[ 0.79464287]
 [ 0.58035713]
 [ 0.6339286 ]
 [ 0.29464287]
 [ 0.75      ]
 [ 0.83035713]
 [ 0.625     ]
 [ 0.44642857]
 [ 0.76785713]
 [ 0.4107143 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.6481961 ]
 [ 0.45385575]
 [ 0.48245132]
 [ 0.32449481]
 [ 0.64276212]
 [ 0.66438043]
 [ 0.60306239]
 [ 0.45833653]
 [ 0.63775462]
 [ 0.53790563]]
Generation 6000: train Loss = 0.09536
Generation 6000: test Loss = 0.10203
 12%|        | 6049/50002 [1:51:10<13:29:18,  1.10s/it]Generation 6050: train Loss = 0.10070
Generation 6050: test Loss = 0.09904
 12%|        | 6100/50002 [1:52:09<18:26:35,  1.51s/it]训练集批次数据的前10个数据
 [[ 0.54464287]
 [ 0.51785713]
 [ 0.41964287]
 [ 0.54464287]
 [ 0.70535713]
 [ 0.52678573]
 [ 0.66071427]
 [ 0.30357143]
 [ 0.4732143 ]
 [ 0.2857143 ]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.52739823]
 [ 0.57507801]
 [ 0.42648035]
 [ 0.48128796]
 [ 0.62464994]
 [ 0.52808595]
 [ 0.61790901]
 [ 0.35251504]
 [ 0.47347325]
 [ 0.42699161]]
测试集批次数据的前10个数据
 [[ 0.4017857 ]
 [ 0.59821427]
 [ 0.53571427]
 [ 0.5625    ]
 [ 0.625     ]
 [ 0.53571427]
 [ 0.6339286 ]
 [ 0.5714286 ]
 [ 0.45535713]
 [ 0.66071427]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.47781879]
 [ 0.57750773]
 [ 0.55734313]
 [ 0.47496486]
 [ 0.50537431]
 [ 0.6551438 ]
 [ 0.60002726]
 [ 0.52313584]
 [ 0.35094139]
 [ 0.53750241]]
Generation 6100: train Loss = 0.09952
Generation 6100: test Loss = 0.09975
 12%|        | 6150/50002 [1:53:05<18:14:04,  1.50s/it]Generation 6150: train Loss = 0.09955
Generation 6150: test Loss = 0.10245
 12%|        | 6199/50002 [1:53:59<13:18:04,  1.09s/it]训练集批次数据的前10个数据
 [[ 0.4107143 ]
 [ 0.3482143 ]
 [ 0.45535713]
 [ 0.6339286 ]
 [ 0.65178573]
 [ 0.6785714 ]
 [ 0.49107143]
 [ 1.        ]
 [ 0.39285713]
 [ 0.66071427]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.46504349]
 [ 0.41659218]
 [ 0.53184867]
 [ 0.57057571]
 [ 0.50991565]
 [ 0.66328168]
 [ 0.57539737]
 [ 0.59945965]
 [ 0.52519053]
 [ 0.68143052]]
测试集批次数据的前10个数据
 [[ 0.60714287]
 [ 0.5089286 ]
 [ 0.3392857 ]
 [ 0.4107143 ]
 [ 0.44642857]
 [ 0.48214287]
 [ 0.44642857]
 [ 0.54464287]
 [ 0.5       ]
 [ 0.5       ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.57818663]
 [ 0.5964483 ]
 [ 0.52238256]
 [ 0.59208608]
 [ 0.53374898]
 [ 0.58853817]
 [ 0.45328   ]
 [ 0.58005309]
 [ 0.46712664]
 [ 0.49189883]]
Generation 6200: train Loss = 0.10446
Generation 6200: test Loss = 0.10245
 12%|        | 6250/50002 [1:54:57<18:46:03,  1.54s/it]Generation 6250: train Loss = 0.10183
Generation 6250: test Loss = 0.10766
 13%|        | 6299/50002 [1:55:51<13:17:51,  1.10s/it]训练集批次数据的前10个数据
 [[ 0.20535715]
 [ 0.54464287]
 [ 0.4017857 ]
 [ 0.41964287]
 [ 0.4017857 ]
 [ 0.5535714 ]
 [ 0.51785713]
 [ 0.53571427]
 [ 0.5535714 ]
 [ 0.48214287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.33588657]
 [ 0.49951231]
 [ 0.62564987]
 [ 0.42560831]
 [ 0.59841794]
 [ 0.57345772]
 [ 0.48575088]
 [ 0.47595826]
 [ 0.57764727]
 [ 0.50679439]]
测试集批次数据的前10个数据
 [[ 0.54464287]
 [ 0.6160714 ]
 [ 0.7410714 ]
 [ 0.6875    ]
 [ 0.42857143]
 [ 0.66071427]
 [ 0.49107143]
 [ 0.54464287]
 [ 0.35714287]
 [ 0.375     ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.49345058]
 [ 0.48770389]
 [ 0.62861991]
 [ 0.514709  ]
 [ 0.49004588]
 [ 0.56163538]
 [ 0.54533452]
 [ 0.56798059]
 [ 0.63588536]
 [ 0.52390897]]
Generation 6300: train Loss = 0.09799
Generation 6300: test Loss = 0.09277
 13%|        | 6350/50002 [1:56:50<18:10:10,  1.50s/it]Generation 6350: train Loss = 0.09502
Generation 6350: test Loss = 0.09063
 13%|        | 6400/50002 [1:57:46<18:13:25,  1.50s/it]训练集批次数据的前10个数据
 [[ 0.29464287]
 [ 0.59821427]
 [ 0.70535713]
 [ 0.30357143]
 [ 0.44642857]
 [ 0.4642857 ]
 [ 0.6964286 ]
 [ 0.5714286 ]
 [ 0.6875    ]
 [ 0.66964287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.47522333]
 [ 0.57326329]
 [ 0.41791487]
 [ 0.32420152]
 [ 0.43559301]
 [ 0.59825456]
 [ 0.5522207 ]
 [ 0.52290827]
 [ 0.56700951]
 [ 0.60182661]]
测试集批次数据的前10个数据
 [[ 0.70535713]
 [ 0.60714287]
 [ 0.54464287]
 [ 0.70535713]
 [ 0.58928573]
 [ 0.66071427]
 [ 0.73214287]
 [ 0.83928573]
 [ 0.73214287]
 [ 0.53571427]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.56113327]
 [ 0.61343205]
 [ 0.48997948]
 [ 0.67675   ]
 [ 0.58045912]
 [ 0.64444184]
 [ 0.65386283]
 [ 0.68307889]
 [ 0.5545978 ]
 [ 0.56402075]]
Generation 6400: train Loss = 0.10494
Generation 6400: test Loss = 0.09435
 13%|        | 6450/50002 [1:58:42<18:46:22,  1.55s/it]Generation 6450: train Loss = 0.11210
Generation 6450: test Loss = 0.09930
 13%|        | 6499/50002 [1:59:36<13:03:32,  1.08s/it]训练集批次数据的前10个数据
 [[ 0.53571427]
 [ 0.54464287]
 [ 0.41964287]
 [ 0.54464287]
 [ 0.5       ]
 [ 0.44642857]
 [ 0.54464287]
 [ 0.53571427]
 [ 0.5625    ]
 [ 0.54464287]]
训练集批次数据通过inference的前10个输出结果
 [[ 0.61624134]
 [ 0.59880537]
 [ 0.49657887]
 [ 0.5747112 ]
 [ 0.43700528]
 [ 0.53706431]
 [ 0.61328274]
 [ 0.55105191]
 [ 0.59434921]
 [ 0.42842034]]
测试集批次数据的前10个数据
 [[ 0.5714286 ]
 [ 0.5714286 ]
 [ 0.4375    ]
 [ 0.8660714 ]
 [ 0.6785714 ]
 [ 0.2857143 ]
 [ 0.53571427]
 [ 0.4017857 ]
 [ 0.42857143]
 [ 0.6339286 ]]
测试集批次数据中通过inference后的前10个输出结果
 [[ 0.46293759]
 [ 0.69227362]
 [ 0.53312361]
 [ 0.65856165]
 [ 0.58635414]
 [ 0.35556024]
 [ 0.63240504]
 [ 0.42244482]
 [ 0.54744732]
 [ 0.6196301 ]]
Generation 6500: train Loss = 0.08970
Generation 6500: test Loss = 0.10122
 13%|        | 6549/50002 [2:00:33<13:21:13,  1.11s/it]Generation 6550: train Loss = 0.09796
Generation 6550: test Loss = 0.10488
 13%|        | 6555/50002 [2:00:41<14:14:41,  1.18s/it]
Process finished with exit code -1
